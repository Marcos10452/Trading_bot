{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa0fe07-420d-4f08-b12f-9a845e21b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://gym-trading-env.readthedocs.io/en/latest/rl_tutorial.html\n",
    "#https://medium.com/@sthanikamsanthosh1994/custom-gym-environment-stock-trading-for-reinforcement-learning-stable-baseline3-629a489d462d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e68878-e414-43eb-bf14-e9f0b4bf500a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8634f79-9ca8-4d8e-b8dc-27618632a225",
   "metadata": {},
   "source": [
    "### Understand the action space\n",
    "Positions\n",
    "I have seen many environments that consider actions such as BUY, SELL. In my experience, it is a mistake to consider a reinforcement learning agent in the same way as a trader. Because, behind a trade, what really matter is the : position reached. In the environment, we label each position by a number : (example with pair BTC/USD)\n",
    "\n",
    "1 : All of our portfolio is converted into BTC. (=BUY ALL)\n",
    "\n",
    "0 : All of our portfolio is converted into USD. (=SELL ALL)\n",
    "\n",
    "Now, we can imagine half position and other variants :\n",
    "\n",
    "0.5 : 50% in BTC & 50% in USD\n",
    "\n",
    "Even : 0.1 : 10% in BTC & 90% in USD …."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f61393-674d-46dd-949a-8c9a674af7aa",
   "metadata": {},
   "source": [
    "# ------------------------------------ Trading Bot --------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15689aa4-3455-4af4-a9fc-1603fb2fc223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import logging #logger\n",
    "#https://www.youtube.com/watch?v=SMZfgeHFFcA\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "#from network import DeepQNetwork\n",
    "#from replay_memory import ReplayBuffer\n",
    "#tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51da5a66-c8e7-432a-bcdb-2184b125a62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1941d1c-fddc-44a7-83c6-061a69f4d7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fd0d2d5-2e43-4383-92c7-ea329a3efd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from env.environment import *\n",
    "from env.portfolio import *\n",
    "from utils.utils import *\n",
    "from agent.agent import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f7ffd43-1fa0-4cf5-a9b8-e569f998e446",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tseries.holiday import USFederalHolidayCalendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bbdb3a7-c9a4-43d7-9db1-525854a80838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 as pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df40612e-9485-4766-ac9a-22799813030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "629e873b-4cba-4b2f-a996-35dad30c7874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# »»»»»»»»»»»»»»»»» STOCKS ««««««««««««««««««««z\n",
    "\n",
    "STOCKS={'amazon':'AMZN','apple':'AAPL','netflix':'NFLX','google':'GOOG','Accenture':'ACN',\n",
    "       'alibaba':'BABA','Turtle Beach':'HEAR','Disney':'DIS',\n",
    "       'LG Display':'LPL','microsoft':'MICS','sony':'SONY',\n",
    "       'Cenovus Energy':'CVE','YPF':'YPF','SHELL':'SHEL','Petrobras':'PBR',\n",
    "       'Coca-Cola':'KO','PespsiCo':'PEP','Unilever':'UL','Kimberly-Clark':'KMB',\n",
    "       'Mondelez':'MDLZ'}\n",
    "\n",
    "#STOCKS = {'amazon':'AMZN'}\n",
    "\n",
    "# dir where parquete file will be created\n",
    "stocks_dir = '/dataset/stocks_aux/'\n",
    "\n",
    "#*****************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8b0bafd-ee3d-43cc-9a5b-a830952f4a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_of_logger():\n",
    "    logger=logging.getLogger()\n",
    "    for handler in logger.handlers[:]:  #make a copy of the list\n",
    "        logger.removeHandler(handler)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f24f8a0a-c1fb-47c2-84d5-eb10d5d7697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "strin_format='%(i)s, %(market_return)s,%(pr)s,%(Sharpe)s,%(score)s,%(avg_score)s,%(best_score)s,%(loss)s,%(n_steps)s,%(pr_val)s,%(score_val)s, %(avg_score_val)s'\n",
    "def initLogging(filename, logger_name):\n",
    "    logger=turn_of_logger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    formatter=logging.Formatter(fmt='%(asctime)s,%(message)s',datefmt='%Y-%m-%d,%H:%M:%S')\n",
    "    #formatter=logging.Formatter(fmt='%(message)s')\n",
    "    fh=logging.FileHandler(filename)\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "    return logger\n",
    "\n",
    "\n",
    "    # sh=logging.StreamHandler(sys.stdout)\n",
    "    # sh.setFormatter(formatter)\n",
    "    # logger.addHandler(sh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceab0bd-7982-4d37-a92a-157616c40fea",
   "metadata": {},
   "source": [
    "### ------------------------------- Reading and preparing data -----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7f603c19-23be-4857-91ed-8f728eb7437b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17528, 55)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training=pd.read_parquet(\"./get_data/data_for_model_v1.parquet\")\n",
    "df_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d26a113e-bac6-4055-9459-cc13f16330aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if index is sorted. if yes ==Ture\n",
    "df_training.datetime.is_monotonic_increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ee3ce73e-f12f-4efb-86a4-e3ea0407c4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>datetime</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>holidays</th>\n",
       "      <th>n_weekday</th>\n",
       "      <th>n_hour</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>...</th>\n",
       "      <th>source10_y</th>\n",
       "      <th>source9_y</th>\n",
       "      <th>source8_y</th>\n",
       "      <th>source7_y</th>\n",
       "      <th>source6_y</th>\n",
       "      <th>source5_y</th>\n",
       "      <th>source4_y</th>\n",
       "      <th>source3_y</th>\n",
       "      <th>source2_y</th>\n",
       "      <th>source1_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-04-06 04:00:00</td>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>04:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>174.91</td>\n",
       "      <td>175.00</td>\n",
       "      <td>173.41</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-04-06 05:00:00</td>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>05:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>173.72</td>\n",
       "      <td>174.10</td>\n",
       "      <td>173.66</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-04-06 06:00:00</td>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>06:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>173.81</td>\n",
       "      <td>173.86</td>\n",
       "      <td>173.21</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-04-06 07:00:00</td>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>07:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>173.33</td>\n",
       "      <td>173.35</td>\n",
       "      <td>172.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-04-06 08:00:00</td>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>08:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>172.69</td>\n",
       "      <td>175.23</td>\n",
       "      <td>172.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker            datetime        date      hour  holidays  n_weekday  \\\n",
       "0   AAPL 2022-04-06 04:00:00  2022-04-06  04:00:00         0          2   \n",
       "1   AAPL 2022-04-06 05:00:00  2022-04-06  05:00:00         0          2   \n",
       "2   AAPL 2022-04-06 06:00:00  2022-04-06  06:00:00         0          2   \n",
       "3   AAPL 2022-04-06 07:00:00  2022-04-06  07:00:00         0          2   \n",
       "4   AAPL 2022-04-06 08:00:00  2022-04-06  08:00:00         0          2   \n",
       "\n",
       "   n_hour    open    high     low  ...  source10_y  source9_y  source8_y  \\\n",
       "0       4  174.91  175.00  173.41  ...         0.0        0.0        0.0   \n",
       "1       5  173.72  174.10  173.66  ...         0.0        0.0        0.0   \n",
       "2       6  173.81  173.86  173.21  ...         0.0        0.0        0.0   \n",
       "3       7  173.33  173.35  172.50  ...         0.0        0.0        0.0   \n",
       "4       8  172.69  175.23  172.00  ...         0.0        0.0        0.0   \n",
       "\n",
       "   source7_y  source6_y  source5_y  source4_y  source3_y  source2_y  source1_y  \n",
       "0        0.0        0.0        0.0        0.0        0.0        0.0        0.0  \n",
       "1        0.0        0.0        0.0        0.0        0.0        0.0        0.0  \n",
       "2        0.0        0.0        0.0        0.0        0.0        0.0        0.0  \n",
       "3        0.0        0.0        0.0        0.0        0.0        0.0        0.0  \n",
       "4        0.0        0.0        0.0        0.0        0.0        0.0        0.0  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4943f69e-e221-407e-b92a-4ab47a1fc0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_training=df_training[(df_training.relevance_score>=0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4de18eb5-c018-4dd6-9f62-098d15563f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_training.loc[0:1,['volume']]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "da20d2ec-acef-414e-a380-191229a07b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Because I am not using all data, I can fill in with other value different from 0. The first value never will be 0.\n",
    "df_training['volume'] = df_training['volume'].replace(0, np.nan).ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "559e6eab-7044-4c35-ac49-358544c61379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:  (2076, 55)\n",
      "True\n",
      "Training:  (3888, 55)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#filter the dates where sentiment is not available\n",
    "\n",
    "#Select Validation first\n",
    "df_validation=df_training[(df_training.date>=datetime.date(2024,1,10))&(df_training.date<=datetime.date(2024,4,25))]\n",
    "print(\"Validation: \",df_validation.shape)\n",
    "#check if index is sorted. if yes ==Ture\n",
    "print(df_validation.datetime.is_monotonic_increasing)\n",
    "# selece Traning second -check to improve this\n",
    "df_training=df_training[(df_training.date>=datetime.date(2023,8,1))&(df_training.date<datetime.date(2024,1,10))]\n",
    "print(\"Training: \",df_training.shape)\n",
    "#check if index is sorted. if yes ==Ture\n",
    "print(df_training.datetime.is_monotonic_increasing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fb524c59-e77f-40f3-a992-e8a3553edc72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>datetime</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>holidays</th>\n",
       "      <th>n_weekday</th>\n",
       "      <th>n_hour</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>...</th>\n",
       "      <th>source10_y</th>\n",
       "      <th>source9_y</th>\n",
       "      <th>source8_y</th>\n",
       "      <th>source7_y</th>\n",
       "      <th>source6_y</th>\n",
       "      <th>source5_y</th>\n",
       "      <th>source4_y</th>\n",
       "      <th>source3_y</th>\n",
       "      <th>source2_y</th>\n",
       "      <th>source1_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15452</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2024-01-10 00:00:00</td>\n",
       "      <td>2024-01-10</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>185.230000</td>\n",
       "      <td>185.480000</td>\n",
       "      <td>185.1500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005196</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15453</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2024-01-10 01:00:00</td>\n",
       "      <td>2024-01-10</td>\n",
       "      <td>01:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>185.230000</td>\n",
       "      <td>185.480000</td>\n",
       "      <td>185.1500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15454</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2024-01-10 02:00:00</td>\n",
       "      <td>2024-01-10</td>\n",
       "      <td>02:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>185.230000</td>\n",
       "      <td>185.480000</td>\n",
       "      <td>185.1500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15455</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2024-01-10 03:00:00</td>\n",
       "      <td>2024-01-10</td>\n",
       "      <td>03:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>185.230000</td>\n",
       "      <td>185.480000</td>\n",
       "      <td>185.1500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15456</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2024-01-10 04:00:00</td>\n",
       "      <td>2024-01-10</td>\n",
       "      <td>04:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>184.880000</td>\n",
       "      <td>185.330000</td>\n",
       "      <td>184.5700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17523</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2024-04-05 15:00:00</td>\n",
       "      <td>2024-04-05</td>\n",
       "      <td>15:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>169.889999</td>\n",
       "      <td>169.979996</td>\n",
       "      <td>169.5000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17524</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2024-04-05 16:00:00</td>\n",
       "      <td>2024-04-05</td>\n",
       "      <td>16:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>169.570000</td>\n",
       "      <td>170.970100</td>\n",
       "      <td>158.8039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17525</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2024-04-05 17:00:00</td>\n",
       "      <td>2024-04-05</td>\n",
       "      <td>17:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>169.560000</td>\n",
       "      <td>169.700000</td>\n",
       "      <td>169.5300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17526</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2024-04-05 18:00:00</td>\n",
       "      <td>2024-04-05</td>\n",
       "      <td>18:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>169.615000</td>\n",
       "      <td>169.680000</td>\n",
       "      <td>169.4500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17527</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2024-04-05 19:00:00</td>\n",
       "      <td>2024-04-05</td>\n",
       "      <td>19:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>169.460000</td>\n",
       "      <td>169.650000</td>\n",
       "      <td>169.3100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2076 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker            datetime        date      hour  holidays  n_weekday  \\\n",
       "15452   AAPL 2024-01-10 00:00:00  2024-01-10  00:00:00         0          2   \n",
       "15453   AAPL 2024-01-10 01:00:00  2024-01-10  01:00:00         0          2   \n",
       "15454   AAPL 2024-01-10 02:00:00  2024-01-10  02:00:00         0          2   \n",
       "15455   AAPL 2024-01-10 03:00:00  2024-01-10  03:00:00         0          2   \n",
       "15456   AAPL 2024-01-10 04:00:00  2024-01-10  04:00:00         0          2   \n",
       "...      ...                 ...         ...       ...       ...        ...   \n",
       "17523   AAPL 2024-04-05 15:00:00  2024-04-05  15:00:00         0          4   \n",
       "17524   AAPL 2024-04-05 16:00:00  2024-04-05  16:00:00         0          4   \n",
       "17525   AAPL 2024-04-05 17:00:00  2024-04-05  17:00:00         0          4   \n",
       "17526   AAPL 2024-04-05 18:00:00  2024-04-05  18:00:00         0          4   \n",
       "17527   AAPL 2024-04-05 19:00:00  2024-04-05  19:00:00         0          4   \n",
       "\n",
       "       n_hour        open        high       low  ...  source10_y  source9_y  \\\n",
       "15452       0  185.230000  185.480000  185.1500  ...         0.0        0.0   \n",
       "15453       1  185.230000  185.480000  185.1500  ...         0.0        0.0   \n",
       "15454       2  185.230000  185.480000  185.1500  ...         0.0        0.0   \n",
       "15455       3  185.230000  185.480000  185.1500  ...         0.0        0.0   \n",
       "15456       4  184.880000  185.330000  184.5700  ...         0.0        0.0   \n",
       "...       ...         ...         ...       ...  ...         ...        ...   \n",
       "17523      15  169.889999  169.979996  169.5000  ...         0.0        0.0   \n",
       "17524      16  169.570000  170.970100  158.8039  ...         0.0        0.0   \n",
       "17525      17  169.560000  169.700000  169.5300  ...         0.0        0.0   \n",
       "17526      18  169.615000  169.680000  169.4500  ...         0.0        0.0   \n",
       "17527      19  169.460000  169.650000  169.3100  ...         0.0        0.0   \n",
       "\n",
       "       source8_y  source7_y  source6_y  source5_y  source4_y  source3_y  \\\n",
       "15452   0.005196        0.0        0.0        0.0        0.0        0.0   \n",
       "15453   0.000000        0.0        0.0        0.0        0.0        0.0   \n",
       "15454   0.000000        0.0        0.0        0.0        0.0        0.0   \n",
       "15455   0.000000        0.0        0.0        0.0        0.0        0.0   \n",
       "15456   0.000000        0.0        0.0        0.0        0.0        0.0   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "17523   0.000000        0.0        0.0        0.0        0.0        0.0   \n",
       "17524   0.000000        0.0        0.0        0.0        0.0        0.0   \n",
       "17525   0.000000        0.0        0.0        0.0        0.0        0.0   \n",
       "17526   0.000000        0.0        0.0        0.0        0.0        0.0   \n",
       "17527   0.000000        0.0        0.0        0.0        0.0        0.0   \n",
       "\n",
       "       source2_y  source1_y  \n",
       "15452        0.0        0.0  \n",
       "15453        0.0        0.0  \n",
       "15454        0.0        0.0  \n",
       "15455        0.0        0.0  \n",
       "15456        0.0        0.0  \n",
       "...          ...        ...  \n",
       "17523        0.0        0.0  \n",
       "17524        0.0        0.0  \n",
       "17525        0.0        0.0  \n",
       "17526        0.0        0.0  \n",
       "17527        0.0        0.0  \n",
       "\n",
       "[2076 rows x 55 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22c43146-3b01-41c7-b8b4-675357ee58ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- arrange columns-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5fa63ace-9a4a-491f-8ab6-520abbf6b198",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1=list(df_training.columns)[2:55] #remove ticker, datetime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0946ec-11c1-4856-b03f-c5a018164ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "88500001-ef18-4069-b744-4e30481cd6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training=df_training[l1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "780cdf26-271f-4ea4-a831-0d647b96865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation=df_validation[l1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0aa33cf2-3449-43fa-a7a8-96755bb47226",
   "metadata": {},
   "outputs": [],
   "source": [
    " #---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9a6eef46-5525-4917-aa39-327e80efb116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_linear_tendecy (x):\n",
    "    return np.ediff1d(x, to_begin=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8a133fff-04d1-4480-92d5-2d61fcf141a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _simple_net_return (x):\n",
    "    \"\"\"\n",
    "        Fractional change between the current and a prior element.\n",
    "        Computes the fractional change from the immediately previous row by default. \n",
    "        This is useful in comparing the fraction of change in a time series of elements.\n",
    "    \"\"\"\n",
    "    result=x.pct_change()\n",
    "    result.fillna(0,inplace=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5daf4d76-544b-4941-98e6-75ccd73226c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _log_return (x):\n",
    "    result=np.log((x/x.shift(1))\n",
    "    result.fillna(0,inplace=True)\n",
    "    return (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "65f21c54-c173-401d-a379-7b897e8ba5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " #---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "885e891a-0abd-4845-8b32-0e434bd286d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #df_training is a DataFrame with columns : \"open\", \"high\", \"low\", \"close\", \"Volume USD\"\n",
    "for dataframe in ['df_training','df_validation']:\n",
    "    #---------------------------------------------------------------------\n",
    "    eval(dataframe)[\"feature_nweek\"] = eval(dataframe)[\"n_weekday\"]\n",
    "    eval(dataframe)[\"feature_holiday\"] =  eval(dataframe)[\"holidays\"]\n",
    "    eval(dataframe)[\"feature_nhour\"] =  eval(dataframe)[\"n_hour\"]\n",
    "    \n",
    "    #Create the feature : ( close[t] - close[t-1] )/ close[t-1]\n",
    "    #df_training[\"diff_close\"] = df_training['close'].diff()\n",
    "    #eval(dataframe)[\"feature_close\"] = ( eval(dataframe)[\"close\"]) /  eval(dataframe)[\"close\"].abs().max() \n",
    "    #eval(dataframe)[\"feature_close\"] = _remove_linear_tendecy(eval(dataframe)[\"close\"]) \n",
    "    #eval(dataframe)[\"feature_close\"] = _simple_net_return(eval(dataframe)[\"close\"]) \n",
    "    eval(dataframe)[\"feature_close\"] = _log_return(eval(dataframe)[\"close\"]) \n",
    "    \n",
    "    # Create the feature : open[t] / close[t]\n",
    "    eval(dataframe)[\"feature_open\"] = _log_return(eval(dataframe)[\"open\"]) \n",
    "    #eval(dataframe)[\"feature_open\"] = ( eval(dataframe)[\"open\"])/  eval(dataframe)[\"close\"].abs().max() \n",
    "    \n",
    "    # Create the feature : high[t] / close[t]\n",
    "    eval(dataframe)[\"feature_high\"] = _log_return(eval(dataframe)[\"high\"]) \n",
    "    #eval(dataframe)[\"feature_high\"] = ( eval(dataframe)[\"high\"])/  eval(dataframe)[\"close\"].abs().max() \n",
    "    \n",
    "    # Create the feature : low[t] / close[t]\n",
    "    eval(dataframe)[\"feature_low\"] = _log_return(eval(dataframe)[\"low\"]) \n",
    "    #eval(dataframe)[\"feature_low\"]= ( eval(dataframe)[\"low\"])/  eval(dataframe)[\"close\"].abs().max() \n",
    "    \n",
    "    #eval(dataframe)[\"feature_volume\"] =  eval(dataframe)[\"volume\"].apply(lambda x: np.log(x) if x!=0 else x)/  eval(dataframe)[\"close\"].abs().max() \n",
    "    eval(dataframe)[\"feature_volume\"] = _log_return(eval(dataframe)[\"volume\"])     \n",
    "    #eval(dataframe)[\"feature_volume\"] =  eval(dataframe)[\"volume\"]/  (eval(dataframe)[\"close\"].abs().max()*1000000)\n",
    "    #create columns for  relevance_score (x) and ticket_sentiment_score (y)\n",
    "    # l1 is from 11 to 55 because I am selecting from score20_x to score1_y\n",
    "    for column in l1[11:55]:\n",
    "        column_name=\"feature_\"+column\n",
    "        eval(dataframe)[column_name]= eval(dataframe)[column] \n",
    "    #---------------------------------------------------------------------\n",
    "    #eval(dataframe).dropna(inplace= True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a379f18f-b5e9-4ace-ae7d-5a56d6d64a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feature_nweek', 'feature_holiday', 'feature_nhour', 'feature_close', 'feature_open', 'feature_high', 'feature_low', 'feature_volume', 'feature_source20_x', 'feature_source19_x', 'feature_source18_x', 'feature_source17_x', 'feature_source16_x', 'feature_source15_x', 'feature_source14_x', 'feature_source13_x', 'feature_source12_x', 'feature_source11_x', 'feature_source10_x', 'feature_source9_x', 'feature_source8_x', 'feature_source7_x', 'feature_source6_x', 'feature_source5_x', 'feature_source4_x', 'feature_source3_x', 'feature_source2_x', 'feature_source1_x', 'feature_source20_y', 'feature_source19_y', 'feature_source18_y', 'feature_source17_y', 'feature_source16_y', 'feature_source15_y', 'feature_source14_y', 'feature_source13_y', 'feature_source12_y', 'feature_source11_y', 'feature_source10_y', 'feature_source9_y', 'feature_source8_y', 'feature_source7_y', 'feature_source6_y', 'feature_source5_y', 'feature_source4_y', 'feature_source3_y', 'feature_source2_y', 'feature_source1_y', 'feature_feature_nweek', 'feature_feature_holiday', 'feature_feature_nhour', 'feature_feature_close']\n"
     ]
    }
   ],
   "source": [
    "feature_list=[ x for x in list(df_training.columns) if \"feature_\" in x] #find all features in columns\n",
    "print(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e8a0e2-0883-466f-97de-d2c266008107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03f892b1-aa71-434f-8676-9f29208ef9ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nan</th>\n",
       "      <th>Null</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>source20_x</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source19_x</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source18_x</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source17_x</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source16_x</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source15_x</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source14_x</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source13_x</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source12_x</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source11_x</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source10_x</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source9_x</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source8_x</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source7_x</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source6_x</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source5_x</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source4_x</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source3_x</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source2_x</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source1_x</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source20_y</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source19_y</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source18_y</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source17_y</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source16_y</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source15_y</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source14_y</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source13_y</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source12_y</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source11_y</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source10_y</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source9_y</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source8_y</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source7_y</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source6_y</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source5_y</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source4_y</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source3_y</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source2_y</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source1_y</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_nweek</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_holiday</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_nhour</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_close</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Nan  Null\n",
       "source20_x         0     0\n",
       "source19_x         0     0\n",
       "source18_x         0     0\n",
       "source17_x         0     0\n",
       "source16_x         0     0\n",
       "source15_x         0     0\n",
       "source14_x         0     0\n",
       "source13_x         0     0\n",
       "source12_x         0     0\n",
       "source11_x         0     0\n",
       "source10_x         0     0\n",
       "source9_x          0     0\n",
       "source8_x          0     0\n",
       "source7_x          0     0\n",
       "source6_x          0     0\n",
       "source5_x          0     0\n",
       "source4_x          0     0\n",
       "source3_x          0     0\n",
       "source2_x          0     0\n",
       "source1_x          0     0\n",
       "source20_y         0     0\n",
       "source19_y         0     0\n",
       "source18_y         0     0\n",
       "source17_y         0     0\n",
       "source16_y         0     0\n",
       "source15_y         0     0\n",
       "source14_y         0     0\n",
       "source13_y         0     0\n",
       "source12_y         0     0\n",
       "source11_y         0     0\n",
       "source10_y         0     0\n",
       "source9_y          0     0\n",
       "source8_y          0     0\n",
       "source7_y          0     0\n",
       "source6_y          0     0\n",
       "source5_y          0     0\n",
       "source4_y          0     0\n",
       "source3_y          0     0\n",
       "source2_y          0     0\n",
       "source1_y          0     0\n",
       "feature_nweek      0     0\n",
       "feature_holiday    0     0\n",
       "feature_nhour      0     0\n",
       "feature_close      0     0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#---------------------------- check for Nulls & NAN _ relevance_score -------------------\n",
    "l1=list(df_training.columns)[11:55]\n",
    "serie_1=df_training[l1].isna().sum()\n",
    "serie_2=df_training[l1].isnull().sum()\n",
    "pd.concat( [serie_1, serie_2], join='outer',keys = ['Nan','Null'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee3bc20c-f325-4dc8-a51b-c47d1603a16d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2076"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check validation notna in close columns\n",
    "df_validation.feature_close.notna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73296e64-5879-41dd-8038-c46d4268ace3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  (3888, 99)\n",
      "Validation:  (2076, 99)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training: \",df_training.shape)\n",
    "print(\"Validation: \",df_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a1b96b-41eb-410e-be1c-d7d3610a5346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cdfde519-11ee-4320-9dab-663ed18aa38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if index is sorted. if yes ==Ture\n",
    "df_training.index.is_monotonic_increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f356631-5aee-4f96-83b7-4f64376ad3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c6a975-b4d7-4aff-87aa-682c00d8b7bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21c151ff-abe9-47fb-a2b6-372c31c739d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check folders to save output\n",
    "import datetime\n",
    "    \n",
    "YEAR        = str(datetime.date.today().year)     # the current year\n",
    "MONTH       = str(datetime.date.today().month)   # the current month\n",
    "DATE        = str(datetime.date.today().day)      # the current day\n",
    "HOUR        = str(datetime.datetime.now().hour)   # the current hour\n",
    "MINUTE      = str(datetime.datetime.now().minute) # the current minute\n",
    "#SECONDS     = datetime.datetime.now().second #the current second\n",
    "string_folder=YEAR+MONTH+DATE+\"_\"+HOUR+MINUTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd45064f-cf25-457c-b990-e2da8fb041ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024416_1523'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1648e6f3-b15c-458f-a348-18fca7c8e67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './output/'+string_folder+'/'\n",
    "if not os.path.exists(PATH):\n",
    "    os.makedirs(PATH)\n",
    "    # Creating folders for best DQN agents\n",
    "    PATH_EVAL=PATH+'agent_best_eval_models'+'/'\n",
    "    PATH_NEXT=PATH+'agent_best_next_models'+'/'\n",
    "    os.makedirs(PATH_EVAL)\n",
    "    os.makedirs(PATH_NEXT)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44cfa6fa-f6f2-4d86-9132-973ba49bc4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logger=initLogging(PATH+\"output.log\",\"output\")\n",
    "logger.info('i,market_return,pr,Sharpe,score,avg_score,best_score,loss,n_steps,pr_val,score_val, avg_score_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "429cacf2-1754-4105-af59-4a1652400359",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_GAMES=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9c12be8d-4441-4d99-a263-117c9952be00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n",
      "[0, 1]\n",
      "No GPU, using /device:CPU:0.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               12800     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79,106\n",
      "Trainable params: 79,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "No GPU, using /device:CPU:0.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 256)               12800     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79,106\n",
      "Trainable params: 79,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcos/.cache/pypoetry/virtualenvs/dqn-kDl4tFs4-py3.10/lib/python3.10/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------\n",
      "episode 0 | MarketReturn -5.53 | PortfolioReturn -12.68 | Sharpe -8.21 | score -12.7 |  avg score -12.7 | best score -inf | loss 1225.2273 | steps 3887\n",
      " PortfolioReturn_val 0.00 |  score_val 0.0 | avg score_val 0.0 \n",
      "-------------------------------------------------------------------------------------------------------\n",
      "... models saved successfully ...\n",
      "Target_net has changed...\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "episode 1 | MarketReturn -5.53 | PortfolioReturn -9.45 | Sharpe -3.71 | score -9.4 |  avg score -11.1 | best score -12.7 | loss 279.9337 | steps 7774\n",
      " PortfolioReturn_val -8.59 |  score_val -8.6 | avg score_val -4.3 \n",
      "-------------------------------------------------------------------------------------------------------\n",
      "... models saved successfully ...\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "episode 2 | MarketReturn -5.53 | PortfolioReturn -11.14 | Sharpe -4.05 | score -11.1 |  avg score -11.1 | best score -9.4 | loss 268.3158 | steps 11661\n",
      " PortfolioReturn_val -8.59 |  score_val -8.6 | avg score_val -5.7 \n",
      "-------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 78\u001b[0m\n\u001b[1;32m     74\u001b[0m actual_obs \u001b[38;5;241m=\u001b[39m new_obs\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m#-----------------------------------------\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m#-----------------------------------------\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# start to train\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m loss\u001b[38;5;241m=\u001b[39m\u001b[43magent_brain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m#------------------------------------------\u001b[39;00m\n\u001b[1;32m     80\u001b[0m n_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/main/Tesis/gym_trading/trading_bot/agent/agent.py:82\u001b[0m, in \u001b[0;36mAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m states, actions, rewards, new_state, dones \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39msample_buffer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[1;32m     81\u001b[0m q_eval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_net\u001b[38;5;241m.\u001b[39mpredict(states)\n\u001b[0;32m---> 82\u001b[0m q_next \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m q_target\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mcopy(q_eval) \u001b[38;5;66;03m#using q_eval as a tamplate. Then, I am going to fill in q_target with belman's equation.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m batch_index\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/dqn-kDl4tFs4-py3.10/lib/python3.10/site-packages/keras/engine/training_v1.py:1057\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_call_args(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1056\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_training_loop(x)\n\u001b[0;32m-> 1057\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/dqn-kDl4tFs4-py3.10/lib/python3.10/site-packages/keras/engine/training_arrays_v1.py:798\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.predict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    788\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    789\u001b[0m     model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    796\u001b[0m ):\n\u001b[1;32m    797\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_validate_or_infer_batch_size(batch_size, steps, x)\n\u001b[0;32m--> 798\u001b[0m     x, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_standardize_user_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predict_loop(\n\u001b[1;32m    802\u001b[0m         model,\n\u001b[1;32m    803\u001b[0m         x,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    808\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/dqn-kDl4tFs4-py3.10/lib/python3.10/site-packages/keras/engine/training_v1.py:2608\u001b[0m, in \u001b[0;36mModel._standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2606\u001b[0m \u001b[38;5;66;03m# Validates `steps` argument based on x's type.\u001b[39;00m\n\u001b[1;32m   2607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_steps:\n\u001b[0;32m-> 2608\u001b[0m     \u001b[43mtraining_utils_v1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_steps_argument\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2610\u001b[0m \u001b[38;5;66;03m# First, we build the model on the fly if necessary.\u001b[39;00m\n\u001b[1;32m   2611\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/dqn-kDl4tFs4-py3.10/lib/python3.10/site-packages/keras/engine/training_utils_v1.py:1510\u001b[0m, in \u001b[0;36mcheck_steps_argument\u001b[0;34m(input_data, steps, steps_name)\u001b[0m\n\u001b[1;32m   1482\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Validates `steps` argument based on input data's type.\u001b[39;00m\n\u001b[1;32m   1483\u001b[0m \n\u001b[1;32m   1484\u001b[0m \u001b[38;5;124;03mThe cases when `steps` value must be provided are when\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;124;03m      but not provided.\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m is_x_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1505\u001b[0m     input_data, (tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator)\n\u001b[1;32m   1506\u001b[0m )\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1508\u001b[0m     input_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m is_x_iterator\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mhas_symbolic_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(input_data, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m input_data)\n\u001b[1;32m   1512\u001b[0m ):\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1514\u001b[0m         input_type_str \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1515\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma Dataset iterator\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_x_iterator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1516\u001b[0m         )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/dqn-kDl4tFs4-py3.10/lib/python3.10/site-packages/keras/engine/training_utils_v1.py:1237\u001b[0m, in \u001b[0;36mhas_symbolic_tensors\u001b[0;34m(ls)\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m   1236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhas_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mls\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/dqn-kDl4tFs4-py3.10/lib/python3.10/site-packages/keras/engine/training_utils_v1.py:1254\u001b[0m, in \u001b[0;36mhas_tensors\u001b[0;34m(ls)\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ls, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m   1251\u001b[0m         tf\u001b[38;5;241m.\u001b[39mis_tensor(v) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, tf\u001b[38;5;241m.\u001b[39mRaggedTensor)\n\u001b[1;32m   1252\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _, v \u001b[38;5;129;01min\u001b[39;00m ls\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1253\u001b[0m     )\n\u001b[0;32m-> 1254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mls\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ls, tf\u001b[38;5;241m.\u001b[39mRaggedTensor)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/dqn-kDl4tFs4-py3.10/lib/python3.10/site-packages/tensorflow/python/framework/tensor_util.py:1030\u001b[0m, in \u001b[0;36mis_tf_type\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1026\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;66;03m# TODO(mdan): Deprecate in favor of more static-friendly types.\u001b[39;00m\n\u001b[0;32m-> 1030\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_tf_type\u001b[39m(x):  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Checks whether `x` is a TF-native type that can be passed to many TF ops.\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m  Use `is_tensor` to differentiate types that can ingested by TensorFlow ops\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;124;03m    `True` if `x` is a TensorFlow-native type.\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(x, internal\u001b[38;5;241m.\u001b[39mNativeObject) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m           \u001b[38;5;28misinstance\u001b[39m(x, core\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m           \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_tensor_like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.compat.v1.disable_eager_execution() #improve \n",
    "    #manage_memory()\n",
    "    #----------------Training----------------\n",
    "    env = TradingEnv(\n",
    "        name= \"AAPL\",\n",
    "        df = df_training, # Your dataset with your custom features\n",
    "        positions = [0, 1], # -1 (=SHORT), 0(=OUT), +1 (=LONG)\n",
    "        portfolio_initial_value=100,\n",
    "        initial_position=1,\n",
    "        \n",
    "        obs_columns=feature_list, #automatically set columns as input\n",
    "        \n",
    "        #trading_fees = 0.01/100, # 0.01% per stock buy / sell (Binance fees)\n",
    "        trading_fees =0,\n",
    "        #borrow_interest_rate= 0.0003/100, # 0.0003% per timestep (one timestep = 1h here)\n",
    "        borrow_interest_rate= 0,\n",
    "        verbose=1\n",
    "    )\n",
    "    #----------------Validation----------------\n",
    "    env_val = TradingEnv(\n",
    "        name= \"AAPL\",\n",
    "        df = df_validation, # Your dataset with your custom features\n",
    "        positions = [0, 1], # -1 (=SHORT), 0(=OUT), +1 (=LONG)\n",
    "        portfolio_initial_value=100,\n",
    "        initial_position=1,\n",
    "\n",
    "        obs_columns=feature_list, #automatically set columns as input\n",
    "        \n",
    "        trading_fees =0,\n",
    "        borrow_interest_rate= 0,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    best_score = -np.inf\n",
    "    load_checkpoint = False\n",
    "    record_agent = False\n",
    "    n_games = NUMBER_GAMES\n",
    "    # Epsilon should be in 0 because I need the agent's brain tries to predidct and it must not select action by random.\n",
    "    agent_brain = Agent(gamma=0.99, epsilon=1, lr=0.001,\n",
    "                  #input_dims=env.observation_space.shape,n_actions=env.action_space.n, \n",
    "                  input_dims=env.observation_space,n_actions=len(env.action_space),\n",
    "                  mem_size=1000000, eps_min=0.01,\n",
    "                  batch_size=64, replace=1000, eps_dec=1e-3, # batch_size=32 ATTENTION\n",
    "                  n_neurons1=256,n_neurons2=256,\n",
    "                  dir=PATH_EVAL, dir2=PATH_NEXT,\n",
    "                  env_name='Trading_bot')\n",
    "    n_steps = 0\n",
    "   \n",
    "    scores, eps_history, steps_array, losses,portfolios,scores_val,portfolios_val = [], [], [], [],[],[],[]    \n",
    "    for i in range(n_games):\n",
    "        done = False\n",
    "        score = 0\n",
    "        terminated = False\n",
    "        truncated =False\n",
    "        done=(terminated|truncated)\n",
    "        #actual_obs=env.reset(seed=7)[0]# I had to do this because array is not two parameters due to new gym version\n",
    "        actual_obs,info=env.reset(seed=7)\n",
    "        while not done: #playing one game.\n",
    "            action = agent_brain.choose_action(actual_obs,True)\n",
    "            #print(\"env.step_action \",action)\n",
    "            new_obs, reward, terminated,truncated,info = env.step(action)\n",
    "            #----------------------------------------------------------------------\n",
    "            #   New GYM version needs to create done from Terminadted and Truncated (after 200)\n",
    "            done=(terminated|truncated)\n",
    "            #-----------------------------------------------------------------------    \n",
    "            score += reward\n",
    "\n",
    "            #print(actual_obs)\n",
    "            agent_brain.store_transition(actual_obs, action,reward, new_obs, done)\n",
    "        \n",
    "            #--------------------------------------\n",
    "            #New observaction is tronsformed to actual_obs\n",
    "            actual_obs = new_obs\n",
    "            #-----------------------------------------\n",
    "            #-----------------------------------------\n",
    "            # start to train\n",
    "            loss=agent_brain.learn()\n",
    "            #------------------------------------------\n",
    "            n_steps += 1\n",
    "            \n",
    "        #----------------------------- Validation ------------------\n",
    "        score_val=0\n",
    "        terminated_val = False\n",
    "        truncated_val =False\n",
    "        done_val=(terminated_val|truncated_val)\n",
    "        actual_obs_val,info_val=env_val.reset(seed=357)\n",
    "        while not done_val: #playing one game.\n",
    "            action_val= agent_brain.choose_action(actual_obs_val,False)\n",
    "            #print(\"env.step_action \",action)\n",
    "            new_obs_val, reward_val, terminated_val,truncated_val,info_val = env_val.step(action_val)\n",
    "            #----------------------------------------------------------------------\n",
    "            #   New GYM version needs to create done from Terminadted and Truncated (after 200)\n",
    "            done_val=(terminated_val|truncated_val)\n",
    "            #-----------------------------------------------------------------------    \n",
    "            score_val+=reward_val\n",
    "\n",
    "        #----------------------------- Metrics ------------------\n",
    "        \n",
    "        eps_history.append(agent_brain.epsilon)\n",
    "        scores.append(score)\n",
    "        scores_val.append(score_val)\n",
    "        losses.append(loss)\n",
    "        #print(\"Score: \", score, \" Loss: \", loss)\n",
    "        steps_array.append(n_steps)\n",
    "        \n",
    "        pr=(100*(env.df.loc[env._idx,'portfolio_valuation'] / env.df.loc[0,'portfolio_valuation'] -1))\n",
    "        portfolios.append(pr)\n",
    "        \n",
    "        pr_val=(100*(env_val.df.loc[env_val._idx,'portfolio_valuation'] / env_val.df.loc[0,'portfolio_valuation'] -1))\n",
    "        portfolios_val.append(pr_val)\n",
    "        \n",
    "        #This is the average score. From the last 100 scores\n",
    "        avg_score = np.mean(scores[-100:])\n",
    "        avg_score_val = np.mean(scores_val[-100:])\n",
    "        market_return=(100*(env.df.loc[env._idx,'close'] / env.df.loc[0,'close'] -1))\n",
    " \n",
    "        #----------------------------sharpe value\n",
    "        Rx=((env.df.iloc[-1,:]['portfolio_valuation']/env.df.iloc[0,:]['portfolio_valuation'])-1)*100\n",
    "        rt=3\n",
    "        Std=np.std(env.df['portfolio_valuation'])\n",
    "        Sharpe=(Rx-rt)/Std\n",
    "        #--------------------------------------------------------------------------------------\n",
    "       \n",
    "        print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "        print('episode {} | MarketReturn {:.2f} | PortfolioReturn {:.2f} | Sharpe {:.2f} | score {:.1f} |  avg score {:.1f} | best score {:.1f} | loss {:.4f} | steps {}'\\\n",
    "              .format(i, market_return,pr,Sharpe,score, avg_score, best_score,loss, n_steps))\n",
    "        print(' PortfolioReturn_val {:.2f} |  score_val {:.1f} | avg score_val {:.1f} '\\\n",
    "              .format(pr_val,score_val, avg_score_val))\n",
    "        print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "        logger.info(f'{i},{market_return},{pr},{Sharpe},{score},{avg_score},{best_score},{loss},{n_steps},{pr_val},{score_val}, {avg_score_val}')\n",
    "        if score > best_score:\n",
    "            if not load_checkpoint:\n",
    "                agent_brain.save_models()\n",
    "            best_score = score\n",
    "        if i % 7 == 0:\n",
    "            agent_brain.target_net.set_weights(agent_brain.policy_net.get_weights())\n",
    "            print('Target_net has changed...')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ece54a-fcc3-4bef-a029-eb5bda45196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "x = [i+1 for i in range(len(scores))]\n",
    "agent_brain.policy_net.save(PATH+'model_q_eval.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a83f996-59ec-430e-b8f0-bc47e28ae79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e1b755-0f3c-4ba7-976a-4f0ea97e1a35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ae114f-f543-4641-b636-cc7181af627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_chart_score(steps_array,scores,PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3333c-7617-4701-b17e-4beb8c88116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_chart_score(steps_array,scores_val,PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833e5757-839a-4e37-8ccc-b5d7ab478a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_chart_loss(steps_array,losses,PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9421fd65-181e-455f-9a63-d6eff2fa5fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_chart_portfolio(steps_array,portfolios,PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e6ea54-0259-4c51-8235-baf954ec6ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.df.to_csv(f\"{PATH}df_{string_folder}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ac9076-5452-4425-9ad8-9ac58fe35c28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0253a19-2dc2-45c1-9649-3c71c273501d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dfa8c4-0ee3-478a-954f-7fa2033ccf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_val.df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eb181d-4f24-4def-97d1-e2b808072ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c5c175-59f7-46d3-ac72-e177b5f99299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d60a69-5380-4f01-b351-2b1736baab0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
